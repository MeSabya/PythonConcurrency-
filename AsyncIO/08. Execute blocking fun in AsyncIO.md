## What is a pool? What is its significance? 

- A pool is just a group of reusable workers (threads or processes).
- Thread pool = a fixed set of threads that can run tasks concurrently.
- Process pool = a fixed set of processes for CPU-heavy tasks.

### When you submit a job:

- If a worker is free ‚Üí it runs immediately.
- If all workers are busy ‚Üí the job waits in a queue until one is free.

### Why Pools are Important

- Limit concurrency (avoid overload)
- Without a pool ‚Üí every time you run blocking code, Python may create a new thread.
- 1000 requests? ‚Üí 1000 threads ‚Üí memory explosion üí•.
- With a pool (say, 20 threads) ‚Üí at most 20 jobs run at once, others wait.
- This protects your system and external services (like SQS/DynamoDB APIs).

### Reuse instead of recreate

- Creating a thread/process is expensive.
- Pools reuse existing workers ‚Üí much faster and more efficient.

### Separation of workloads

- You can create multiple pools for different types of jobs.
- Example:
    - Pool A: 5 threads for SQS calls.
    - Pool B: 10 threads for database writes.
    - Pool C: 4 processes for CPU-heavy image resizing.

This prevents one workload from starving others.

### Asyncio + Pools

- Asyncio itself runs in one thread (the event loop).
- Great for I/O bound async tasks (HTTP, DB queries).
- But when you call a blocking function (like boto3, file I/O, CPU math), the event loop freezes.

üëâ To fix this, you offload blocking tasks to a pool:

- ThreadPoolExecutor ‚Üí for I/O blocking tasks (file I/O, boto3, slow APIs).
- ProcessPoolExecutor ‚Üí for CPU-heavy tasks (image processing, ML inference).

üìù Example (Why Pool Matters)
Without a Pool (too many threads)

```python
import asyncio
import time
from threading import Thread

def blocking_task(i):
    time.sleep(1)
    print(f"Task {i} done")

async def main():
    tasks = [asyncio.to_thread(blocking_task, i) for i in range(1000)]
    await asyncio.gather(*tasks)

asyncio.run(main())
```

‚ö†Ô∏è This may create 1000 threads ‚Äî memory spike, context switching overhead.
With a Pool (safe concurrency)

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

def blocking_task(i):
    time.sleep(1)
    return f"Task {i} done"

async def main():
    loop = asyncio.get_running_loop()
    # pool with only 10 threads
    with ThreadPoolExecutor(max_workers=10) as pool:
        futures = [
            loop.run_in_executor(pool, blocking_task, i)
            for i in range(1000)
        ]
        results = await asyncio.gather(*futures)
        print(results[:10])

asyncio.run(main())
```


- ‚úÖ Only 10 threads are ever active.
- ‚úÖ New tasks wait in a queue until a thread is free.
- ‚úÖ Stable memory usage, predictable performance.

### üéØ Intuition

Think of a pool like the number of counters in a bank üè¶:
- No pool: every customer gets their own counter ‚Üí chaos, building full of counters, inefficient.
- Pool: fixed 5 counters ‚Üí at most 5 customers served at once, others wait in line ‚Üí controlled, predictable.

## to_thread vs run_in_executor 

üëâ What run_in_executor can do that to_thread cannot
Choose a custom executor

- to_thread ‚Üí always uses the global default thread pool (shared across your program).
- run_in_executor ‚Üí lets you pass your own ThreadPoolExecutor or ProcessPoolExecutor.

```python
loop.run_in_executor(my_thread_pool, blocking_func, arg)
loop.run_in_executor(my_process_pool, cpu_heavy_func, arg)
```

### ‚úÖ Why useful?

You can give different workloads different pools.

Example:

- Use one pool with 5 threads for SQS API calls.
- Another pool with 20 threads for disk writes.
- Another pool as a ProcessPool for CPU-heavy JSON parsing.

With to_thread, everything fights for the same default thread pool.
run_in_executor ‚Üí you can create a pool, set max_workers, and explicitly .shutdown() it.

### Summary

#### ‚úÖ Use to_thread when:

- You just need a quick way to run blocking I/O in threads.
- You don‚Äôt care about custom pools or processes.
- Example: wrapping a boto3 call in async code.

#### ‚úÖ Use run_in_executor when:

- You need custom thread pool size.
- You want separate pools for different workloads.
- You need a ProcessPool (CPU-bound tasks).
- You want fine-grained lifecycle control of your pools.


### Let‚Äôs create a production-ready asyncio setup with multiple executors for different job types.

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time
import random

# --- Blocking task examples ---
def sqs_call(message_id: int):
    """Simulate an SQS network call (I/O bound)"""
    print(f"[SQS] Processing message {message_id}")
    time.sleep(random.uniform(0.5, 1.5))
    return f"SQS done {message_id}"

def db_write(record_id: int):
    """Simulate a database write (I/O bound)"""
    print(f"[DB] Writing record {record_id}")
    time.sleep(random.uniform(0.2, 0.7))
    return f"DB done {record_id}"

def image_resize(image_id: int):
    """Simulate CPU-bound image resizing"""
    print(f"[Image] Resizing image {image_id}")
    # Simulate CPU work
    total = sum(i*i for i in range(10_000_000))
    return f"Image done {image_id}"

# --- Async wrapper to submit tasks to executors ---
async def run_in_executor(executor, func, *args):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(executor, func, *args)

# --- Main async function ---
async def main():
    # Define pools
    sqs_pool = ThreadPoolExecutor(max_workers=5)
    db_pool = ThreadPoolExecutor(max_workers=10)
    cpu_pool = ProcessPoolExecutor(max_workers=4)

    try:
        tasks = []

        # Submit SQS jobs
        for i in range(10):
            tasks.append(run_in_executor(sqs_pool, sqs_call, i))

        # Submit DB jobs
        for i in range(10):
            tasks.append(run_in_executor(db_pool, db_write, i))

        # Submit CPU-heavy jobs
        for i in range(4):
            tasks.append(run_in_executor(cpu_pool, image_resize, i))

        # Gather results
        results = await asyncio.gather(*tasks)
        for res in results:
            print(res)
    finally:
        # Properly shutdown executors
        sqs_pool.shutdown(wait=True)
        db_pool.shutdown(wait=True)
        cpu_pool.shutdown(wait=True)

if __name__ == "__main__":
    asyncio.run(main())
```

