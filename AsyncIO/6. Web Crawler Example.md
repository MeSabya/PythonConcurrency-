From what we have learnt so far, asyncio is an excellent choice for blocking operations. Usually, there are two kinds of blocking operations:

network I/O

disk I/O

Let's start with implementing a simple web crawler. A web crawler is a program that systematically browses the world wide web, typically with the intent to index it. For our purposes, we'll dumb down our crawler and limit its capability to fetch the HTML for a list of URLs. The downloaded HTML is passed onto a consumer which then performs the indexing but we'll not implement that part.

The meat of the problem lies in asynchronously downloading the given URLs. We'll be using the aiohttp module for asynchronous REST GET calls. If we were to serially download the URLs, we'll unnecessarily be wasting CPU cycles as a network request is usually the slowest form of I/O. Let's start with the serial version of the code.

## Code using native co routine

```python
import asyncio
import aiohttp
import time


async def crawl_one_url(url, session):
    get_request = session.get(url)

    res = await get_request
    txt = await res.text()

    get_request.close()

    return txt


async def crawl_urls(urls_to_crawl):
    session = aiohttp.ClientSession()

    work_to_do = list()
    for url in urls_to_crawl:
        work_to_do.append(crawl_one_url(url, session))

    res = await asyncio.gather(*work_to_do)

    await session.close()
    return res


def main():
    t0 = time.time()

    urls_to_crawl = get_urls_to_crawl()

    asyncio.run(crawl_urls(urls_to_crawl))
    elapsed = time.time() - t0
    print("\n{} URLS downloaded in {:.2f}s".format(len(urls_to_crawl), elapsed))


def get_urls_to_crawl():
    urls_list = list()
    urls_list.append('http://www.cnn.com/')
    urls_list.append('https://www.foxnews.com/')
    urls_list.append('https://www.bbc.com/')
    urls_list.append('https://www.dawn.com')
    urls_list.append('https://www.cnbc.com')
    urls_list.append('https://www.twitter.com')

    return urls_list


if __name__ == '__main__':
    main()
```

## Code using generator based coroutine

```python
import asyncio
import aiohttp
import time


@asyncio.coroutine
def crawl_one_url(url, session):
    get_request = session.get(url)

    res = yield from get_request
    txt = yield from res.text()

    get_request.close()
    return txt


@asyncio.coroutine
def crawl_urls(urls_to_crawl):
    session = aiohttp.ClientSession()

    work_to_do = list()
    for url in urls_to_crawl:
        work_to_do.append(crawl_one_url(url, session))

    completed, pending = yield from asyncio.wait(work_to_do, return_when=asyncio.ALL_COMPLETED)

    # uncomment to retrieve the downloaded HTML
    # for task in completed:
    #     print(task.result())

    # remember to clean up
    yield from session.close()


def main():
    urls_to_crawl = get_urls_to_crawl()
    start = time.time()

    loop = asyncio.get_event_loop()
    loop.run_until_complete(crawl_urls(urls_to_crawl))
    elapsed = time.time() - start
    print("\n{} URLS downloaded in {:.2f}s".format(len(urls_to_crawl), elapsed))


def get_urls_to_crawl():
    urls_list = list()
    urls_list.append('http://www.cnn.com/')
    urls_list.append('https://www.foxnews.com/')
    urls_list.append('https://www.bbc.com/')
    urls_list.append('https://www.dawn.com')
    urls_list.append('https://www.cnbc.com')
    urls_list.append('https://www.twitter.com')

    return urls_list


if __name__ == '__main__':
    main()
```

## code using multi threading 

```python
import time
from urllib.request import urlopen
from threading import Thread


def get_urls_to_crawl():
    urls_list = list()
    urls_list.append('http://www.cnn.com/')
    urls_list.append('https://www.foxnews.com/')
    urls_list.append('https://www.bbc.com/')
    urls_list.append('https://www.dawn.com')
    urls_list.append('https://www.cnbc.com')
    urls_list.append('https://www.twitter.com')
    return urls_list


def crawl_one_url(url):
    html = urlopen(url)
    text = html.read()


if __name__ == "__main__":

    urls_to_crawl = get_urls_to_crawl()
    start = time.time()

    threads = list()
    for url in get_urls_to_crawl():
        threads.append(Thread(target=crawl_one_url, args=(url,)))

    for thread in threads:
        thread.start()

    for thread in threads:
        thread.join()

    elapsed = time.time() - start
    print("\n{} URLS downloaded in {:.2f}s".format(len(urls_to_crawl), elapsed))
  ```

